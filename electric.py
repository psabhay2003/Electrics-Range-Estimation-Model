# -*- coding: utf-8 -*-
"""Electric.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t0rM9RTtk6vu-w5OMGc9Y3vAytP4tkdc
"""

#import the necessary libraries and load the datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
train = pd.read_csv('https://raw.githubusercontent.com/psabhay2003/Electrics-Range-Estimation-Model/refs/heads/main/Train.csv')
test = pd.read_csv('https://raw.githubusercontent.com/psabhay2003/Electrics-Range-Estimation-Model/refs/heads/main/Test.csv')

train.head()

test.head()

#data summary
train.info()
train.describe()
train.isnull().sum()

#dropping irrelevant columns
columns_to_drop = ["County", "City", "State", "Postal Code", "Legislative District", "DOL Vehicle ID"]
train = train.drop(columns=columns_to_drop, axis=1)
print(train.columns)
test = test.drop(columns=columns_to_drop, axis=1)
print(test.columns)

#Outlier Handeling and Normalisation
outlier_features = train.select_dtypes(include=np.number).columns
# Boxplot for Outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=train
 [outlier_features])
plt.title("Boxplot Showing Outliers in Features")
plt.xticks(rotation=45)
plt.show()
#this difference in 2020 Census Tract is because the features are not scaled

y_train = train["Electric_Range"] #y_train is target variable
X_train = train.drop(columns=["Electric_Range"])
X_test  = test.copy()
numeric_cols     = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X_train.select_dtypes(include=["object", "bool", "category"]).columns.tolist()

from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    # map unseen categories in test to -1
    X_test[col] = X_test[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)
    label_encoders[col] = le

#1) Identify numeric columns (after encoding)
numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()

#2) Outlier capping on numeric cols (1stâ€“99th percentile)
for col in ["Base MSRP", "Electric_Range"]:
    lower, upper = train[col].quantile([0.01, 0.99])
    train[col] = train[col].clip(lower, upper)


#3) Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# Ensure that both X_train and X_test have the same columns for scaling
# Get the common columns present in both DataFrames
common_cols = list(set(X_train.columns) & set(X_test.columns))

# Select only common numeric columns for scaling
numeric_cols_to_scale = [col for col in numeric_cols if col in common_cols]
X_train[numeric_cols_to_scale] = scaler.fit_transform(X_train[numeric_cols_to_scale])
X_test[numeric_cols_to_scale]  = scaler.transform(X_test[numeric_cols_to_scale])

# Plot the boxplot AFTER scaling
plt.figure(figsize=(12, 6))
# Select only the common columns present in both X_train and outlier_features
common_cols = list(set(X_train.columns) & set(outlier_features))
sns.boxplot(data=X_train[common_cols])
plt.title("Boxplot After Scaling")
plt.xticks(rotation=45)
plt.show()

#to check if all of the features are now numeric
X_train.info()

#feature engineering
#deriving new feature such as cost per mile using base MSRP and Electric Range
import math
def calculate_cost_per_mile(row):
  base_MSRP = row['Base MSRP']
  Electric_Range = row['Electric_Range']
  Cost_per_Mile = base_MSRP / Electric_Range
  return Cost_per_Mile
train["Cost_per_Mile"]=train.apply(calculate_cost_per_mile,axis=1)
X_train["Cost_per_Mile"] = train["Cost_per_Mile"]
X_test["Cost_per_Mile"] = test.apply(calculate_cost_per_mile, axis=1)
X_train["Cost_per_Mile"]

#feature engineering
#deriving new feature such as Vehicle Age using Model Year
from datetime import datetime

current_year = 2025
train["Vehicle_Age"] = current_year - train["Model Year"]
X_train["Vehicle_Age"] = train["Vehicle_Age"]
test["Vehicle_Age"] = current_year - test["Model Year"]
X_test["Vehicle_Age"] = test["Vehicle_Age"]
X_train["Vehicle_Age"]

# Ensure both have the same columns (and remove potentially problematic ones)
features = train.columns.tolist()  # Get the columns from X_train
features.remove('Electric_Range')   # Remove 'Electric_Range' from features
features.remove('Unnamed: 0')       # Remove 'Unnamed: 0' from features (if present)

# Use the common features for training and prediction
X_train = X_train[features]
X_test = X_test[features]
#model training
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_train_pred = rf_model.predict(X_train)
mae = mean_absolute_error(y_train, y_train_pred)
mse = mean_squared_error(y_train, y_train_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_train, y_train_pred)
rmse

test_predictions = rf_model.predict(X_test)
test_predictions

# Create the directory if it doesn't exist
import os
os.makedirs("/mnt/data", exist_ok=True)

# Create submission file
submission = pd.DataFrame({"Id": range(len(test_predictions)), "Predicted_Rating": test_predictions})
submission.to_csv("/mnt/data/submission.csv", index=False)

print("Submission file created successfully!")

from google.colab import files
files.download("/mnt/data/submission.csv")